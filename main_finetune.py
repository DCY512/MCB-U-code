#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import argparse
import os
import time
import random
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

import models.convnextv2 as convnextv2
import models.convnextv1 as convnextv1

from engine_finetune import train_one_epoch, evaluate, SimpleEMA
import utils as U
import datasets as D
import json
import pandas as pd
from models.modules.losses import DistillationLoss
import csv # ç¡®ä¿ csv å·²è¢«å¯¼å…¥
import inspect
from models.convnextv2_dual import ConvNeXtV2Dual
import torch.multiprocessing as mp



def build_base_criterion(args) -> nn.Module:
    """æ ¹æ® --base_loss é€‰æ‹©åŸºç¡€ç›‘ç£æŸå¤±ï¼›ä¿æŒå¤šæ ‡ç­¾åœºæ™¯é»˜è®¤ BCEã€‚"""
    if args.base_loss == 'bce':
        return nn.BCEWithLogitsLoss()

    elif args.base_loss == 'focal':
        # ä½ é¡¹ç›®é‡Œå·²æœ‰å®ç°ï¼šmodels/modules/custom_losses/focal_loss.py
        from models.modules.custom_losses.focal_loss import FocalLoss
        return FocalLoss(gamma=args.focal_gamma, alpha=args.focal_alpha)
    
    elif args.base_loss == 'mlsm':
        # è¾“å…¥åº”ä¸º logitsï¼ˆä¸ BCEWithLogitsLoss ç›¸åŒä¹ æƒ¯ï¼‰ï¼Œè¯„ä¼°é˜¶æ®µä¼šè‡ªè¡Œ sigmoid
        return nn.MultiLabelSoftMarginLoss()

    elif args.base_loss == 'fals':
        # ä½ é¡¹ç›®é‡Œå·²æœ‰å®ç°ï¼šmodels/modules/custom_losses/fals_loss.py
        from models.modules.custom_losses.fals_loss import FALSLoss
        return FALSLoss(eps=args.fals_eps, gamma=args.fals_gamma, reduction='mean')

    elif args.base_loss == 'mcb':
        # ä½ é¡¹ç›®é‡Œå·²æœ‰å®ç°ï¼šmodels/modules/custom_losses/mcb_loss.py
        from models.modules.custom_losses.mcb_loss import MCBLoss
        return MCBLoss(momentum=args.mcb_momentum, reduction='mean')

    elif args.base_loss == 'dals':
        from models.modules.custom_losses.dals_loss import DALSBCE
        return DALSBCE(eps=args.dals_eps, gamma=args.dals_gamma)

    elif args.base_loss == 'mcb_convex':
        from models.modules.custom_losses.mcb_loss import MCBLossConvex
        return MCBLossConvex(tau=args.mcb_tau, w_min=args.mcb_wmin, momentum=args.mcb_momentum)

    elif args.base_loss == 'gebce':
        from models.modules.custom_losses.gebce import GEBCELoss
        return GEBCELoss(
            lambda_coef=args.ge_lambda,
            pos_only=args.ge_pos_only,
            alpha=args.ge_alpha,
            ema=args.ge_ema,
            momentum=args.ge_momentum,
            band=args.ge_band,
            trainable=args.ge_trainable,   # â† æ–°å¢
        )


    else:
        # å…œåº•ï¼ˆå‘åå…¼å®¹ï¼‰
        return nn.BCEWithLogitsLoss()



def append_summary_to_global_log(args, best_metric_value, metric_name, model_total_params, 
                                 class_names, per_class_ap_list):
    """
    å°†æœ¬æ¬¡å®éªŒçš„æœ€ç»ˆæ€»ç»“ï¼ˆåŒ…å« per-class APï¼‰ï¼Œè¿½åŠ å†™å…¥åˆ°å…¨å±€æ—¥å¿—æ–‡ä»¶ä¸­ã€‚
    """
    summary_file_path = Path("experiments_summary_test_2.csv")
    
    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘åŠ¨æ€æ„å»ºè¡¨å¤´ ---
    headers = [
        'output_dir', 'model', 'aug_mode', 'fuse_mode', 'ahcr_mode', 'attention_config',
        'best_metric_name', 'best_metric_value', 'total_params_M', 'batch_size', 'learning_rate',
    ]
    # ä¸ºæ¯ä¸ªç±»åˆ«éƒ½åŠ¨æ€æ·»åŠ ä¸€ä¸ª AP åˆ—
    if class_names and per_class_ap_list:
        ap_headers = [f"AP_{name.replace(' ', '_')}" for name in class_names]
        headers.extend(ap_headers)
    # --------------------------------

    summary_data = {
        'output_dir': args.output_dir,
        'model': args.model,
        'aug_mode': args.aug_mode,
        'fuse_mode': args.fuse_mode,
        'ahcr_mode': args.ahcr_mode if args.fuse_mode == 'ahcr' else 'N/A',
        'attention_config': args.attention_config if args.attention_config else '{}',
        'best_metric_name': metric_name,
        'best_metric_value': f"{best_metric_value:.4f}",
        'total_params_M': f"{model_total_params / 1_000_000:.2f}",
        'batch_size': args.batch_size,
        'learning_rate': args.lr,
    }
    
    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘å°† per-class AP æ•°æ®æ·»åŠ åˆ°è¦å†™å…¥çš„è¡Œä¸­ ---
    if class_names and per_class_ap_list and len(class_names) == len(per_class_ap_list):
        for i, name in enumerate(class_names):
            summary_data[f"AP_{name.replace(' ', '_')}"] = f"{per_class_ap_list[i]:.4f}"
    # ----------------------------------------------------

    try:
        file_exists = summary_file_path.is_file()
        with open(summary_file_path, 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            if not file_exists:
                writer.writeheader()
            writer.writerow(summary_data)
        print(f"ğŸ“ˆ æœ€ç»ˆç»“æœï¼ˆå«Per-Class APï¼‰å·²æˆåŠŸè¿½åŠ åˆ°æ€»æˆç»©è¡¨: {summary_file_path}")
    except Exception as e:
        print(f"âŒ å†™å…¥æ€»æˆç»©è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")


def set_seed(seed: int = 42):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False


def _load_finetune_weights(model: nn.Module, ckpt_path: str, prefix: str = '') -> None:
    if not ckpt_path:
        return
    p = Path(ckpt_path)
    if not p.exists():
        print(f"[finetune] file not found: {ckpt_path}")
        return

    sd = None
    if p.suffix == ".safetensors":
        from safetensors.torch import load_file
        sd = load_file(str(p))
    else:
        obj = torch.load(str(p), map_location="cpu")
        sd = obj["model"] if (isinstance(obj, dict) and "model" in obj) else obj

    cleaned = {}
    for k, v in sd.items():
        nk = k
        if nk.startswith("module."):
            nk = nk[len("module."):]
        if prefix and nk.startswith(prefix):
            nk = nk[len(prefix):]
        cleaned[nk] = v

    try:
        mount = getattr(args, "model_mount", "")
    except NameError:
        mount = ""
    if mount:
        cleaned = {(mount + k): v for k, v in cleaned.items()}

    msd = model.state_dict()
    to_load = {}
    skipped_shape = []
    for k, v in cleaned.items():
        if k in msd and tuple(v.shape) == tuple(msd[k].shape):
            to_load[k] = v
        elif k in msd:
            skipped_shape.append((k, tuple(v.shape), tuple(msd[k].shape)))

    msg = model.load_state_dict(to_load, strict=False)
    print(f"[finetune] loaded={len(to_load)}  skipped_shape={len(skipped_shape)}  "
          f"missing={len(msg.missing_keys)}  unexpected={len(msg.unexpected_keys)}")
    
    # === æ‰“å°ç¼ºå¤±/æ„å¤–é”®ï¼ˆæœ€å¤šå‰ 200 æ¡ï¼Œé¿å…åˆ·å±ï¼‰ ===
    _missing = list(getattr(msg, 'missing_keys', []))
    _unexpected = list(getattr(msg, 'unexpected_keys', []))

    if _missing:
        print("[finetune] missing keys (first 200):")
        for k in _missing[:200]:
            print("  -", k)

    if _unexpected:
        print("[finetune] unexpected keys (first 200):")
        for k in _unexpected[:200]:
            print("  -", k)

    # === æ–¹ä¾¿æ’æŸ¥ï¼šæŠŠå®Œæ•´æ¸…å•ä¿å­˜åˆ°è¾“å‡ºç›®å½•ï¼ˆè‹¥èƒ½è·å–ï¼‰ ===
    # å°è¯•ä»ç¯å¢ƒå˜é‡æˆ–å¸¸è§å˜é‡é‡Œæ‹¿ output_dirï¼›æ‹¿ä¸åˆ°å°±è½åˆ°å½“å‰ç›®å½•
    out_dir = os.environ.get("OUTPUT_DIR_HINT", "")
    try:
        # å¦‚æœä¸»ç¨‹åºåœ¨è°ƒç”¨å‰è®¾ç½®è¿‡ args.output_dirï¼Œè¿™é‡Œä¹Ÿè®¸èƒ½å–åˆ°
        out_dir = out_dir or getattr(globals().get('args', None), 'output_dir', '')
    except Exception:
        pass

    save_root = out_dir if out_dir else "."
    try:
        os.makedirs(save_root, exist_ok=True)
        with open(os.path.join(save_root, "finetune_missing_keys.txt"), "w") as f:
            for k in _missing:
                f.write(k + "\n")
        with open(os.path.join(save_root, "finetune_unexpected_keys.txt"), "w") as f:
            for k in _unexpected:
                f.write(k + "\n")
        print(f"[finetune] å·²å°†ç¼ºå¤±/æ„å¤–é”®æ¸…å•å†™å…¥åˆ°: {save_root}/finetune_*_keys.txt")
    except Exception as e:
        print(f"[finetune] âš ï¸ ä¿å­˜ç¼ºå¤±/æ„å¤–é”®æ¸…å•å¤±è´¥: {e}")

    if skipped_shape:
        print("[finetune] first few shape-mismatch keys:")
        for i, (k, s_ckpt, s_model) in enumerate(skipped_shape[:10]):
            print(f"  - {k}: ckpt{s_ckpt} vs model{s_model}")


def _try_build_loaders_with_project(args):
    builder_names = ["build_loaders", "build_dataloaders", "create_loaders", "create_dataloaders"]
    for name in builder_names:
        if hasattr(D, name):
            return getattr(D, name)(args)
    if hasattr(D, "XrayMultiLabelList"):
        train_ds = D.XrayMultiLabelList(args.train_list, args.classes_file, is_train=True,
                                        dual_view=args.dual_view, input_size=args.input_size)
        val_ds = D.XrayMultiLabelList(args.val_list, args.classes_file, is_train=False,
                                      dual_view=args.dual_view, input_size=args.input_size)
        
        # ä¼˜åŒ–åçš„ DataLoader é…ç½®ï¼Œå……åˆ†åˆ©ç”¨å…±äº«å†…å­˜
        train_loader = DataLoader(
            train_ds, 
            batch_size=args.batch_size, 
            shuffle=True,
            num_workers=args.num_workers,
            pin_memory=True,  # å…³é”®ï¼šå¯ç”¨å›ºå®šå†…å­˜
            prefetch_factor=2,  # é¢„å–æ‰¹æ¬¡
            persistent_workers=True,  # ä¿æŒworkerè¿›ç¨‹
            multiprocessing_context='spawn',  # ä½¿ç”¨spawnæ–¹å¼
            drop_last=True
        )
        val_loader = DataLoader(
            val_ds, 
            batch_size=args.batch_size, 
            shuffle=False,
            num_workers=args.num_workers,
            pin_memory=True,
            prefetch_factor=2,
            persistent_workers=True,
            multiprocessing_context='spawn'
        )
        return train_loader, val_loader, getattr(train_ds, "class_names", None)
    raise RuntimeError("datasets.py ç¼ºå°‘æ„å»ºå‡½æ•°ï¼ˆbuild_loaders/...ï¼‰ï¼Œè¯·ä¿ç•™å·¥ç¨‹é‡Œçš„æ•°æ®é›†é€»è¾‘ã€‚")


def get_args_parser():
    parser = argparse.ArgumentParser(add_help=True)
    # basic
    parser.add_argument('--model', default='convnextv2_base')
    parser.add_argument('--input_size', default=384, type=int)
    parser.add_argument('--batch_size', default=8, type=int)
    parser.add_argument('--epochs', default=100, type=int)
    parser.add_argument('--lr', default=1e-3, type=float)
    parser.add_argument('--min_lr', default=1e-6, type=float)
    parser.add_argument('--warmup_epochs', default=5, type=int)
    parser.add_argument('--weight_decay', default=0.05, type=float)
    parser.add_argument('--drop_path', default=0.2, type=float)
    parser.add_argument('--output_dir', default='./output')
    parser.add_argument('--device', default='cuda')
    parser.add_argument('--seed', default=42, type=int)
    parser.add_argument('--num_workers', type=int, default=8)

    # init weights
    parser.add_argument('--finetune', default='')
    parser.add_argument('--model_prefix', default='', help="åŠ å‰ç¼€")
    parser.add_argument("--model_mount", type=str, default="", help="å¯é€‰ï¼šç»Ÿä¸€æŒ‚åœ¨åˆ°æŸå­æ¨¡å—å‰")

    # data
    parser.add_argument('--dual_view', default=True, type=U.str2bool)
    parser.add_argument('--train_list', default='annotations/DvXray_train.txt')
    parser.add_argument('--val_list',   default='annotations/DvXray_val.txt')
    parser.add_argument('--classes_file', default='annotations/classes.txt')
    parser.add_argument('--num_classes', default=15, type=int)
    parser.add_argument('--multi_label', default=True, type=U.str2bool)
    parser.add_argument('--eval_threshold', default=0.5, type=float)

    # backbone feats
    parser.add_argument('--return_intermediate', default=False, type=U.str2bool)
    parser.add_argument('--out_indices', default=[1,2,3], nargs='+', type=int)

    # dual-fuse headï¼ˆæ–°å¢ xattn é€‰é¡¹ + å‚æ•°ï¼‰
    parser.add_argument('--fuse_mode', default='concat', choices=['concat', 'add', 'gated', 'xattn', 'ahcr'])
    parser.add_argument('--fuse_levels', default=['C3','C4','C5'], nargs='+')
    # dual-fuse headï¼ˆæ–°å¢ fpn_fuse é€‰é¡¹ï¼‰
    parser.add_argument('--head_type', default='c5', type=str, help="Type of head for feature aggregation: c5, fpn, fpn_fuse, fpn_pan")
    parser.add_argument('--attention_config', type=str, default=None,
                    help='JSON string for complex attention configurations. '
                         'Example: \'{"N3": "eca", "N4": ["se", "cbam"]}\'')

    parser.add_argument('--xattn_heads', default=4, type=int, help='ä»… fuse_mode=xattn æ—¶ä½¿ç”¨')
    parser.add_argument('--xattn_reduction', default=4, type=int, help='ç©ºé—´ä¸‹é‡‡æ ·å› å­ï¼Œ2/4/8')
    parser.add_argument('--fpn_out_channels', default=256, type=int, help='FPNè¾“å‡ºé€šé“æ•°')  # æ–°å¢å‚æ•°
    
    # ... å…¶ä»–å‚æ•° ...
    parser.add_argument('--patience', type=int, default=0,
                    help='Enable early stopping if validation metric does not improve for this many epochs. '
                         'Default 0 to disable.')
    parser.add_argument('--ahcr_mode', default='intra_level', choices=['intra_level', 'inter_level'],
                        help="Defines the hierarchical strategy for AHCR fusion. Only used if fuse_mode is 'ahcr'.")
    # teacher/EMA
    parser.add_argument('--teacher_mode', default=True, type=U.str2bool)
    parser.add_argument('--ema_decay', default=0.9999, type=float)
    parser.add_argument('--ema_device', default='cpu')
    parser.add_argument('--fsdp_cpu_offload', default=False, type=U.str2bool)
    # ç»­è®­å‚æ•°
    parser.add_argument('--resume', default='', help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ (checkpoint_last.pth æˆ– checkpoint_best.pth)')
    parser.add_argument('--resume_epoch', default=-1, type=int, help='ä»æŒ‡å®šepochå¼€å§‹ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼‰')
    parser.add_argument('--resume_optimizer', default=True, type=U.str2bool, help='æ˜¯å¦æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€')
    parser.add_argument('--resume_scheduler', default=True, type=U.str2bool, help='æ˜¯å¦æ¢å¤å­¦ä¹ ç‡è°ƒåº¦å™¨')


    # è’¸é¦
    parser.add_argument('--use_distillation', type=U.str2bool, default=False)
    parser.add_argument('--teacher_model', type=str, default='convnext_small')
    parser.add_argument('--teacher_weights', type=str, default='')
    parser.add_argument('--kd_mode', type=str, default='logits', choices=['logits', 'dkd'])
    parser.add_argument('--distillation_alpha', type=float, default=0.5, help="Hard loss weight.")
    parser.add_argument('--distillation_tau', type=float, default=2.0)
    parser.add_argument('--distill_feature_layers', type=str, nargs='+', default=None)
    parser.add_argument('--distillation_beta', type=float, default=0.0, help="Feature loss weight.")
    parser.add_argument('--dkd_alpha', type=float, default=1.0)
    parser.add_argument('--dkd_beta', type=float, default=8.0)

    # --- Base Loss é€‰æ‹©ï¼ˆé»˜è®¤ bceï¼Œå‘åå…¼å®¹ï¼‰ ---
    parser.add_argument('--base_loss', type=str, default='bce',
                    choices=['bce','mlsm','focal','fals','mcb','gebce','dals','mcb_convex'],
                    help='é€‰æ‹©åŸºç¡€ç›‘ç£æŸå¤±ï¼šbce / mlsm / focal / fals / mcb / gebce / dals / mcb_convex')

    # Focal Loss è¶…å‚
    parser.add_argument('--focal_gamma', type=float, default=2.0)
    parser.add_argument('--focal_alpha', type=float, default=None)  # å¯ä¸º None æˆ– float

    # FALSï¼ˆç„¦ç‚¹å¯¹æŠ—æ€§æ ‡ç­¾å¹³æ»‘ï¼‰è¶…å‚
    parser.add_argument('--fals_eps', type=float, default=0.1)
    parser.add_argument('--fals_gamma', type=float, default=2.0)

    # MCBï¼ˆå…ƒåŠ æƒç±»åˆ«å¹³è¡¡ï¼‰è¶…å‚
    parser.add_argument('--mcb_momentum', type=float, default=0.9)

    # --- GE-BCE è¶…å‚ï¼ˆé»˜è®¤ç¨³å¥å€¼ï¼‰ ---
    parser.add_argument('--ge_lambda', type=float, default=0.1,
                        help='GE-BCE: class-level gradient equalization strength')
    parser.add_argument('--ge_pos_only', type=U.str2bool, default=True,
                        help='GE-BCE: use positives only to compute G_c')
    parser.add_argument('--ge_alpha', type=float, default=0.75,
                        help='GE-BCE: weight for positives when pos_only is False')
    parser.add_argument('--ge_ema', type=U.str2bool, default=True,
                        help='GE-BCE: EMA smoothing over G_c')
    parser.add_argument('--ge_momentum', type=float, default=0.9,
                        help='GE-BCE: EMA momentum')
    parser.add_argument('--ge_band', type=float, default=0.0,
                        help='GE-BCE: tolerance band; diffs within band are not penalized')

    # DALSï¼ˆå‡¸ç‰ˆ FALSï¼‰
    parser.add_argument('--dals_eps',   type=float, default=0.1)
    parser.add_argument('--dals_gamma', type=float, default=2.0)

    # MCBï¼ˆå‡¸åŒ–ï¼‰
    parser.add_argument('--mcb_tau',  type=float, default=1.0)
    parser.add_argument('--mcb_wmin', type=float, default=1e-3)

    # GE-BCEï¼šæ˜¯å¦è®©æ­£åˆ™å‚ä¸åä¼ ï¼ˆtrue=éå‡¸ï¼›false=ä»…è¯Šæ–­ï¼‰
    parser.add_argument('--ge_trainable', type=U.str2bool, default=False,
                        help='GE æ­£åˆ™æ˜¯å¦å‚ä¸åä¼ ï¼ˆtrue=éå‡¸ï¼›false=ä»…è¯Šæ–­ï¼‰')
    # æ•°æ®å¢å¼º
    parser.add_argument('--aug_mode', default='standard',
                    choices=['none','standard',
                             'conditional','conditional_1','conditional_2','conditional_3','conditional_4',
                             'rand_aug','trivial_aug'],
                    help="Select data augmentation strategy.")
    parser.add_argument('--rand_aug_n', type=int, default=2, help="Hyperparameter N for RandAugment.") 
    parser.add_argument('--rand_aug_m', type=int, default=9, help="Hyperparameter M for RandAugment (0-30).")
    # CSV eval
    parser.add_argument('--eval_csv', default='')
    # æ–°å¢ï¼šè¯„ä¼°æ—¶å°† per-class AP åˆ—æŒ‰ AP å€¼æ’åºå†™å…¥ CSVï¼ˆé»˜è®¤ Falseï¼‰
    parser.add_argument('--eval_csv_sort_classes', default=False, type=U.str2bool,
                        help="If true, sort per-class AP columns by descending AP when writing CSV.")
    # æ–°å¢ï¼šæ¯ä¸ªæ ·æœ¬è¾“å‡º top-k é¢„æµ‹ï¼ˆ0 è¡¨ç¤ºä¸è¾“å‡ºï¼‰
    parser.add_argument('--eval_csv_topk', default=0, type=int,
                        help="If >0, write per-sample top-k predictions (class:score) to a separate CSV per epoch.")
    return parser


def build_model(args):
    

    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ™ºèƒ½æ¨¡å‹æ„å»ºé€»è¾‘ ---
    backbone_builder = None
    # 1. ä¼˜å…ˆåœ¨ convnextv2 æ¨¡å—ä¸­æŸ¥æ‰¾
    if hasattr(convnextv2, args.model):
        backbone_builder = getattr(convnextv2, args.model)
        print(f"âœ… ä» [ConvNeXtV2] æ¨¡å—ä¸­æˆåŠŸæ‰¾åˆ°æ¨¡å‹æ„å»ºå™¨: {args.model}")
    # 2. å¦‚æœ V2 ä¸­æ²¡æœ‰ï¼Œå†å» convnextv1 æ¨¡å—ä¸­æŸ¥æ‰¾
    elif hasattr(convnextv1, args.model):
        backbone_builder = getattr(convnextv1, args.model)
        print(f"âœ… ä» [ConvNeXtV1] æ¨¡å—ä¸­æˆåŠŸæ‰¾åˆ°æ¨¡å‹æ„å»ºå™¨: {args.model}")
    
    if backbone_builder is None:
        raise ValueError(f"é”™è¯¯: åœ¨ models/convnextv2.py æˆ– models/convnextv1.py ä¸­å‡æœªæ‰¾åˆ°åä¸º '{args.model}' çš„æ¨¡å‹å‡½æ•°ã€‚")
    # -------------------------------------------

    # å°è¯•ç”¨æœ€å°‘çš„å‚æ•°åˆå§‹åŒ–backboneï¼Œä»¥é¿å…å†²çª
    # è¿™å¯¹äºåŠ è½½ V1 çš„ timm é£æ ¼æ¨¡å‹å¾ˆé‡è¦
    try:
        backbone = backbone_builder(num_classes=0)
    except TypeError:
        backbone = backbone_builder()

    sig = inspect.signature(ConvNeXtV2Dual.__init__)
    valid_keys = set(sig.parameters.keys())

    out_idx = tuple(getattr(args, "out_indices", (1, 2, 3)))
    fuse_kw = getattr(args, "fuse_mode", "add")
    
    # --- ã€å…³é”®ä¿®æ­£ã€‘æ¸…ç†äº†æ—§çš„ã€æ— ç”¨çš„æ³¨æ„åŠ›è§£æé€»è¾‘ ---
    # ç°åœ¨ç›´æ¥ä½¿ç”¨ head_type
    base_head_type = getattr(args, "head_type", "c5")
    
    # è§£æ JSON æ ¼å¼çš„æ³¨æ„åŠ›é…ç½®
    parsed_attention_config = None
    if getattr(args, "attention_config", None):
        try:
            parsed_attention_config = json.loads(args.attention_config)
            print("âœ… æˆåŠŸè§£ææ³¨æ„åŠ›é…ç½®:", parsed_attention_config)
        except json.JSONDecodeError:
            raise ValueError(f"é”™è¯¯: è§£æ --attention_config çš„ JSON å­—ç¬¦ä¸²å¤±è´¥: {args.attention_config}")
    
    # æ„é€ ä¼ é€’ç»™ ConvNeXtV2Dual çš„å‚æ•°å­—å…¸
    candidate_kwargs = {
        "backbone": backbone,

        "num_classes": args.num_classes,
        "fuse_mode": fuse_kw,
        "return_intermediate": getattr(args, "return_intermediate", False),
        "out_indices": out_idx,
        "fuse_levels": getattr(args, "fuse_levels", None),
        "head_type": base_head_type,
        "xattn_heads": getattr(args, "xattn_heads", 4),
        "xattn_reduction": getattr(args, "xattn_reduction", 4),
        "fpn_out_channels": getattr(args, "fpn_out_channels", 256),
        "attention_config": parsed_attention_config,
        "ahcr_mode": getattr(args, "ahcr_mode", "intra_level"),
    }
    
    # è¿‡æ»¤æ‰ä¸åœ¨ ConvNeXtV2Dual.__init__ å‚æ•°åˆ—è¡¨ä¸­çš„é”®
    filtered = {k: v for k, v in candidate_kwargs.items() if k in valid_keys}

    model = ConvNeXtV2Dual(**filtered)
    return model


def _safe_evaluate(data_loader_val, model_to_eval, device, amp=True, class_names=None, threshold=None, csv_path=None):
    try:
        return evaluate(data_loader_val, model_to_eval, device, amp=amp,
                        class_names=class_names, threshold=threshold, csv_path=csv_path)
    except TypeError:
        try:
            return evaluate(data_loader_val, model_to_eval, device, amp=amp,
                            class_names=class_names, threshold=threshold)
        except TypeError:
            try:
                return evaluate(data_loader_val, model_to_eval, device, amp=amp)
            except TypeError:
                return evaluate(data_loader_val, model_to_eval, device)

def _load_resume_checkpoint(model, optimizer, model_ema, checkpoint_path, args):
    """åŠ è½½ç»­è®­æ£€æŸ¥ç‚¹"""
    if not checkpoint_path or not os.path.isfile(checkpoint_path):
        return 0, -1.0, 0
    
    print(f"ğŸ“‚ åŠ è½½ç»­è®­æ£€æŸ¥ç‚¹: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # åŠ è½½æ¨¡å‹æƒé‡
    model.load_state_dict(checkpoint['model'])
    
    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
    if args.resume_optimizer and 'optimizer' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer'])
        print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²æ¢å¤")
    
    # åŠ è½½EMAæ¨¡å‹çŠ¶æ€
    if model_ema is not None and 'model_ema' in checkpoint:
        # éœ€è¦é‡æ–°åˆ›å»ºEMAçŠ¶æ€
        model_ema.ema_state = checkpoint['model_ema']
        print("âœ… EMAæ¨¡å‹çŠ¶æ€å·²æ¢å¤")
    
    # è·å–èµ·å§‹epochå’Œæœ€ä½³æŒ‡æ ‡
    start_epoch = checkpoint.get('epoch', 0) + 1
    best_metric = checkpoint.get('metric', {}).get('mAP', -1.0)
    # --- [æ—©åœ] ä»æ£€æŸ¥ç‚¹åŠ è½½æ—©åœè®¡æ•°å™¨ ---
    epochs_since_best = checkpoint.get('epochs_since_best', 0)
    
    print(f"âœ… ä» {checkpoint_path} æˆåŠŸç»­è®­ã€‚")
    print(f"   - èµ·å§‹è½®æ¬¡: {start_epoch}")
    print(f"   - å·²è¾¾æœ€ä½³ mAP: {best_metric:.4f}")
    print(f"   - æ—©åœè®¡æ•°å™¨çŠ¶æ€: {epochs_since_best}")

    # --- [æ—©åœ] è¿”å›åŒ…å«è®¡æ•°å™¨çš„æ–°å…ƒç»„ ---
    return start_epoch, best_metric, epochs_since_best





def main(args):
    print(args)
    set_seed(args.seed)
    device = torch.device(args.device)
    
    # è®¾ç½®å…±äº«å†…å­˜ç­–ç•¥
    
    try:
        mp.set_sharing_strategy('file_system')
        print("âœ… å…±äº«å†…å­˜ç­–ç•¥è®¾ç½®ä¸º 'file_system'")
    except Exception as e:
        print(f"âš ï¸ è®¾ç½®å…±äº«å†…å­˜ç­–ç•¥æ—¶å‡ºé”™: {e}")
    
    # è®¾ç½® PyTorch å†…å­˜ä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.max_split_size_mb = 512
    
    # å¯ç”¨ CUDA å†…å­˜ä¼˜åŒ–
    if torch.cuda.is_available():
        torch.cuda.set_per_process_memory_fraction(0.9)  # ä½¿ç”¨ 90% GPU å†…å­˜
        print("âœ… CUDA å†…å­˜ä¼˜åŒ–å·²å¯ç”¨")

    loaders = _try_build_loaders_with_project(args)
    if len(loaders) == 3:
        data_loader_train, data_loader_val, class_names = loaders
    else:
        data_loader_train, data_loader_val = loaders
        class_names = None

# --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘é‡æ„çš„ã€é€»è¾‘æ­£ç¡®çš„æ¨¡å‹ä¸æŸå¤±åˆ›å»ºæµç¨‹ ---

    # 1. æ°¸è¿œå…ˆåˆ›å»ºå­¦ç”Ÿæ¨¡å‹
    print(" assembling student model...")


    # å…ˆæ„å»ºå­¦ç”Ÿæ¨¡å‹...ï¼ˆä¿æŒä¸å˜ï¼‰
    model = build_model(args).to(device)
    if args.finetune:
        print(f"Load pre-trained student weights from: {args.finetune}")
        _load_finetune_weights(model, args.finetune, prefix=args.model_prefix or '')

    # === æ–°å¢ï¼šæ— è®ºæ˜¯å¦è’¸é¦ï¼Œå…ˆæ„å»ºâ€œåŸºç¡€ç›‘ç£æŸå¤±â€ ===
    base_criterion = build_base_criterion(args)
    print(f"[BaseLoss] Using {args.base_loss}  ->  {base_criterion.__class__.__name__}")

    if args.use_distillation:
        print("ğŸ”¥ Knowledge distillation mode enabled!")
        # ç»„è£…æ•™å¸ˆæ¨¡å‹ï¼ˆä¿æŒä½ åŸæ¥çš„é€»è¾‘ï¼‰
        teacher_build_args = argparse.Namespace(**vars(args))
        teacher_build_args.model = args.teacher_model
        teacher_model = build_model(teacher_build_args).to(device)

        if args.teacher_weights:
            print(f"   - Loading teacher weights from: {args.teacher_weights}")
            _load_finetune_weights(teacher_model, args.teacher_weights)
        else:
            print("   - âš ï¸ WARNING: No teacher weights provided. Teacher will use random weights.")

        # ç”¨â€œåŸºç¡€ç›‘ç£æŸå¤±â€ä½œä¸ºè’¸é¦åŒ…è£…é‡Œçš„ base_criterion
        from models.modules.losses import DistillationLoss
        criterion = DistillationLoss(
            base_criterion=base_criterion,     # â˜… å…³é”®ï¼šè¿™é‡Œæ¢æˆå¯é€‰çš„åŸºç¡€æŸå¤±
            student_model=model,
            teacher_model=teacher_model,
            kd_mode=args.kd_mode,
            alpha=args.distillation_alpha,
            beta=args.distillation_beta,
            dkd_alpha=args.dkd_alpha,
            dkd_beta=args.dkd_beta,
            tau=args.distillation_tau,
            feature_layers=args.distill_feature_layers,
            adapter_configs=None,
        )
        print(f"   - DistillationLoss ready (kd_mode={args.kd_mode}, alpha={args.distillation_alpha}, tau={args.distillation_tau})")
    else:
        print("ğŸ”· Standard training mode.")
        # éè’¸é¦ç›´æ¥ç”¨åŸºç¡€æŸå¤±
        criterion = base_criterion

    print("Criterion =", criterion.__class__.__name__)


    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    model_ema = None
    if args.teacher_mode:
        model_ema = SimpleEMA(model, decay=args.ema_decay, device=args.ema_device)
        print(f"[teacher_mode] EMA enabled with decay={args.ema_decay}, device={args.ema_device}")

    # ========== ç»­è®­é€»è¾‘ ==========
    start_epoch = 0
    best_metric = -1.0
    # --- [æ—©åœ] åˆå§‹åŒ–æ—©åœè®¡æ•°å™¨ ---
    epochs_since_best = 0
    
    if args.resume:
        checkpoint_path = args.resume
        # æ™ºèƒ½å¤„ç†è·¯å¾„ï¼šå¦‚æœæä¾›çš„æ˜¯ç›®å½•ï¼Œåˆ™è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
        if os.path.isdir(checkpoint_path):
            last_ckpt = Path(checkpoint_path) / "checkpoint_last.pth"
            best_ckpt = Path(checkpoint_path) / "checkpoint_best.pth"
            if last_ckpt.exists():
                checkpoint_path = str(last_ckpt)
                print(f"æ£€æµ‹åˆ°ç›®å½•ï¼Œä½¿ç”¨æœ€æ–°çš„æ£€æŸ¥ç‚¹: {checkpoint_path}")
            elif best_ckpt.exists():
                checkpoint_path = str(best_ckpt)
                print(f"æ£€æµ‹åˆ°ç›®å½•ï¼Œä½¿ç”¨æœ€ä½³çš„æ£€æŸ¥ç‚¹: {checkpoint_path}")
            else:
                # å¦‚æœç›®å½•ä¸ºç©ºï¼Œåˆ™ä¸åŠ è½½ä»»ä½•ä¸œè¥¿ï¼Œè¡Œä¸ºç­‰åŒäºä¸ç»­è®­
                print(f"âš ï¸ ç»­è®­ç›®å½• {args.resume} ä¸ºç©ºï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
                checkpoint_path = None 

        # åªæœ‰åœ¨ç¡®å®šäº†æœ‰æ•ˆçš„æ£€æŸ¥ç‚¹æ–‡ä»¶åï¼Œæ‰è¿›è¡ŒåŠ è½½
        if checkpoint_path and os.path.isfile(checkpoint_path):
            start_epoch, best_metric, epochs_since_best = _load_resume_checkpoint(
                model, optimizer, model_ema, checkpoint_path, args
            )
        
        # å…è®¸å‘½ä»¤è¡Œå‚æ•°è¦†ç›–ä»æ£€æŸ¥ç‚¹ä¸­è¯»å–çš„epoch
        if args.resume_epoch >= 0:
            print(f"æ‰‹åŠ¨è¦†ç›–èµ·å§‹è½®æ¬¡ä¸º: {args.resume_epoch}")
            start_epoch = args.resume_epoch
    # =============================

    best_val_stats = {}
    metric_name = "mAP"
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ========== æ·»åŠ CSVæ—¥å¿—æ–‡ä»¶ ==========
    import csv
    csv_path = output_dir / "training_log.csv"
    
    # ç»­è®­æ—¶ä»¥è¿½åŠ æ¨¡å¼æ‰“å¼€CSVï¼Œå¦åˆ™æ–°å»º
    if start_epoch == 0:
        with open(csv_path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['epoch', 'train_loss', 'val_metric', 'best_metric',
                 'epoch_time', 'avg_epoch_time', 'estimated_remaining_hours',
                 'completion_time', 'grad_var'])
        print("ğŸ“ åˆ›å»ºæ–°çš„è®­ç»ƒæ—¥å¿—æ–‡ä»¶")
    else:
        print(f"ğŸ“ ç»­è®­æ¨¡å¼ï¼Œå°†è¿½åŠ åˆ°ç°æœ‰æ—¥å¿—æ–‡ä»¶: {csv_path}")
    # ====================================

    # ========== æ·»åŠ æ—¶é—´é¢„ä¼°ä»£ç  ==========
    import datetime
    start_time = time.time()
    epoch_times = []
    
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒï¼Œæ€»è½®æ¬¡: {args.epochs}, èµ·å§‹è½®æ¬¡: {start_epoch}")
    # --- [æ—©åœ] æ‰“å°æ—©åœçŠ¶æ€ ---
    if args.patience > 0:
        print(f"âŒ› æ—©åœæœºåˆ¶å·²å¯ç”¨ï¼Œè€å¿ƒå€¼ (Patience) = {args.patience} è½®")
    print(f"ğŸ“Š è®­ç»ƒé›† batches/epoch: {len(data_loader_train)}")
    # ====================================

    start = time.time()
    for epoch in range(start_epoch, args.epochs):
         # ====== conditional_1ï¼šåŠ¨æ€å¢å¼ºå¼ºåº¦ ======
        if args.aug_mode == "conditional_1":
            ds = getattr(data_loader_train, "dataset", None)
            if ds is not None and hasattr(ds, "set_epoch"):
                ds.set_epoch(epoch, args.epochs)
        epoch_start = time.time()  # è®°å½•epochå¼€å§‹æ—¶é—´
        
        train_stats = train_one_epoch(
            model=model,
            criterion=criterion,
            data_loader=data_loader_train,
            optimizer=optimizer,
            device=device,
            epoch=epoch,
            amp=True,
            model_ema=model_ema,
        )

        if model_ema is not None:
            # åˆ›å»ºä¸´æ—¶æ¨¡å‹ç”¨äºEMAè¯„ä¼°
            eval_model = build_model(args).to(device)
            model_ema.copy_to(eval_model)
            print("ğŸ“Š ä½¿ç”¨EMAæ¨¡å‹è¿›è¡Œè¯„ä¼°")
        else:
            eval_model = model
            print("ğŸ“Š ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œè¯„ä¼°")
        
        # ========== ç§»é™¤å•ä¸ªepochçš„eval CSVç”Ÿæˆ ==========
        val_stats = _safe_evaluate(
            data_loader_val=data_loader_val,
            model_to_eval=eval_model,
            device=device,
            amp=True,
            class_names=class_names,
            threshold=args.eval_threshold,
            csv_path=None,  # è®¾ç½®ä¸ºNoneï¼Œä¸ç”Ÿæˆå•ä¸ªCSV
        )
        # ====== conditional_2ï¼šæ ¹æ®éªŒè¯é›† AP æ›´æ–°å¼±ç§‘ç±» ======
        if args.aug_mode == "conditional_2":
            ds = getattr(data_loader_train, "dataset", None)
            if ds is not None and hasattr(ds, "update_hard_classes_by_ap"):
                per_class_ap = val_stats.get("per_class_ap", None)
                if per_class_ap is not None:
                    val_ap_dict = {i: float(ap) for i, ap in enumerate(per_class_ap)}
                    ds.update_hard_classes_by_ap(val_ap_dict, topk=3)

        # ========== æ·»åŠ æ—¶é—´é¢„ä¼°è®¡ç®—å’Œæ‰“å° ==========
        epoch_time = time.time() - epoch_start
        epoch_times.append(epoch_time)
        
        # è®¡ç®—å¹³å‡epochæ—¶é—´å’Œå‰©ä½™æ—¶é—´é¢„ä¼°
        avg_epoch_time = sum(epoch_times) / len(epoch_times)
        remaining_epochs = args.epochs - epoch - 1
        estimated_remaining = avg_epoch_time * remaining_epochs
        completion_time = datetime.datetime.now() + datetime.timedelta(seconds=estimated_remaining)
        
        print(f"â° Epoch {epoch} è€—æ—¶: {epoch_time:.1f}s, å¹³å‡: {avg_epoch_time:.1f}s, å‰©ä½™é¢„ä¼°: {estimated_remaining/3600:.1f}h")
        print(f"  é¢„è®¡å®Œæˆ: {completion_time.strftime('%m-%d %H:%M')}")
        # ==========================================

        primary = None
        if isinstance(val_stats, dict):
            for k in ["mAP", "map", "AP", "ap", "acc1", "acc", "top1"]:
                if k in val_stats:
                    primary = float(val_stats[k]); metric_name = k; break
        if primary is None:
            primary = -float(val_stats.get("loss", train_stats.get("loss", 0.0))) if isinstance(val_stats, dict) else -float(train_stats.get("loss", 0.0))
            metric_name = "-loss"

        is_best = primary > best_metric
        
        if is_best: 
            best_metric = primary
             # --- [æ—©åœ] å¦‚æœæ˜¯æœ€ä½³ï¼Œé‡ç½®è®¡æ•°å™¨ ---
            best_val_stats = val_stats 
            epochs_since_best = 0
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æ€§èƒ½! mAP = {best_metric:.4f}. é‡ç½®æ—©åœè®¡æ•°å™¨ã€‚")
        else:
            # --- [æ—©åœ] å¦‚æœä¸æ˜¯æœ€ä½³ï¼Œè®¡æ•°å™¨+1 ---
            epochs_since_best += 1
            print(f"ğŸ“‰ æ€§èƒ½æœªæå‡ï¼Œæ—©åœè®¡æ•°å™¨: {epochs_since_best}/{args.patience}")


        # ========== å†™å…¥CSVæ—¥å¿— ==========
        with open(csv_path, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                epoch,
                train_stats.get('loss', 0.0),
                primary,
                best_metric,
                epoch_time,
                avg_epoch_time,
                estimated_remaining/3600,
                completion_time.strftime('%m-%d %H:%M'),
                train_stats.get('grad_var', 0.0)
            ])
        # ================================

        ckpt = {
            "model": model.state_dict(), 
            "optimizer": optimizer.state_dict(), 
            "args": vars(args),
            "epoch": epoch, 
            "metric": {metric_name: best_metric},
            "epochs_since_best": epochs_since_best
        }
        
        # ä¿å­˜EMAæ¨¡å‹çŠ¶æ€
        if model_ema is not None:
            ckpt["model_ema"] = model_ema.ema_state
        
        torch.save(ckpt, str(output_dir / "checkpoint_last.pth"))
        if is_best:
            torch.save(ckpt, str(output_dir / "checkpoint_best.pth"))

        took = time.time() - start
        print(f"[epoch {epoch}] val {metric_name}={primary:.4f} (best={best_metric:.4f})   elapsed={took/60.0:.1f} min")
        # --- [æ—©åœ] æ£€æŸ¥æ˜¯å¦è§¦å‘æ—©åœ ---
        if args.patience > 0 and epochs_since_best >= args.patience:
            print(f"\nğŸ›‘ è§¦å‘æ—©åœ! éªŒè¯é›†æŒ‡æ ‡å·²è¿ç»­ {args.patience} è½®æœªæå‡ã€‚")
            print(f"   - æœ€ä½³æ€§èƒ½å‡ºç°åœ¨ç¬¬ {epoch - epochs_since_best} è½®ï¼Œ{metric_name} = {best_metric:.4f}")
            break  # ä¸­æ–­è®­ç»ƒå¾ªç¯

    # ========== æ·»åŠ è®­ç»ƒå®Œæˆæ€»è€—æ—¶ ==========
    total_time = time.time() - start_time
    print(f"\nâœ… è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
    
    # ========== æŒ‰mAPæ’åºCSVæ–‡ä»¶ ==========
    print("ğŸ“Š æŒ‰mAPæ’åºè®­ç»ƒæ—¥å¿—...")
    
    try:
        df = pd.read_csv(csv_path)
        df_sorted = df.sort_values('val_metric', ascending=False)  # æŒ‰val_metric(mAP)é™åºæ’åº
        df_sorted.to_csv(csv_path, index=False)
        print(f"âœ… è®­ç»ƒæ—¥å¿—å·²æŒ‰mAPæ’åºå¹¶ä¿å­˜è‡³: {csv_path}")
    except Exception as e:
        print(f"âš ï¸ æ’åºCSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
    # =====================================
    

    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘è°ƒç”¨æ–°å‡½æ•°ï¼Œå¹¶ä¼ é€’ per-class AP ---
    try:
        total_params = sum(p.numel() for p in model.parameters())
    except:
        total_params = 0
    
    # ä»æˆ‘ä»¬ä¿å­˜çš„æœ€ä½³ç»Ÿè®¡æ•°æ®ä¸­è·å– per_class_ap
    best_per_class_ap = best_val_stats.get('per_class_ap', [])

    append_summary_to_global_log(
        args=args, 
        best_metric_value=best_metric, 
        metric_name=metric_name,
        model_total_params=total_params,
        class_names=class_names, # class_names æ˜¯ä» build_loaders è·å–çš„
        per_class_ap_list=best_per_class_ap
    )
    # --------------------------------------------------

    print("Finished.")


if __name__ == "__main__":
    parser = get_args_parser()
    args = parser.parse_args()
    main(args)



