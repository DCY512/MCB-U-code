# models/modules/losses.py (å®Œæ•´æ›¿æ¢)

import torch
import torch.nn as nn
from .distillation.kd_losses import LogitsDistillationLoss, DKDLoss, FeatureLoss
from .distillation.predictors import FeatureExtractor
from typing import List, Dict, Optional


class DistillationLoss(nn.Module):
    """
    çŸ¥è¯†è’¸é¦æŸå¤±â€œæ€»è°ƒåº¦ä¸­å¿ƒâ€ (v2, æ”¯æŒç‰¹å¾è’¸é¦)ã€‚
    """
    def __init__(self, base_criterion: nn.Module, student_model: nn.Module, teacher_model: nn.Module,
                 kd_mode: str = 'logits', alpha: float = 0.5, beta: float = 1.0,
                 dkd_alpha: float = 1.0, dkd_beta: float = 8.0,
                 tau: float = 2.0, class_weights=None, 
                 feature_layers: List[str] = None, adapter_configs: Dict = None):
        super().__init__()
        self.base_criterion = base_criterion
        self.student_model = student_model
        self.teacher_model = teacher_model
        self.kd_mode = kd_mode
        self.alpha = alpha # å¹³è¡¡å› å­ (ç¡¬æŸå¤±)
        self.beta = beta   # å¹³è¡¡å› å­ (ç‰¹å¾æŸå¤±)

        # --- Logits / DKD è’¸é¦å™¨ ---
        if kd_mode == 'logits':
            self.logits_distiller = LogitsDistillationLoss(tau=tau, class_weights=class_weights)
        elif kd_mode == 'dkd':
            self.logits_distiller = DKDLoss(alpha=dkd_alpha, beta=dkd_beta, temperature=tau)
        else:
            self.logits_distiller = None
        
        # --- ç‰¹å¾è’¸é¦å™¨ ---
        self.feature_distiller = None
        if feature_layers:
            print("ğŸ”¥ ç‰¹å¾è’¸é¦å·²å¯ç”¨!")
            student_extractor = FeatureExtractor(student_model.backbone, feature_layers)
            teacher_extractor = FeatureExtractor(teacher_model.backbone, feature_layers)
            self.feature_distiller = FeatureLoss(student_extractor, teacher_extractor, adapter_configs)
            
        self.teacher_model.eval()
        for param in self.teacher_model.parameters():
            param.requires_grad = False

    def forward(self, student_outputs, student_inputs, targets):
        xa, xb = student_inputs
        
        hard_loss = self.base_criterion(student_outputs, targets)
        
        soft_loss = 0.0
        if self.logits_distiller:
            with torch.no_grad():
                teacher_logits = self.teacher_model(xa, xb) if xb is not None else self.teacher_model(xa)
                if isinstance(teacher_logits, dict): teacher_logits = teacher_logits['logits']

            if self.kd_mode == 'dkd':
                soft_loss = self.logits_distiller(student_outputs, teacher_logits, targets)
            else: # 'logits'
                soft_loss = self.logits_distiller(student_outputs, teacher_logits)
        
        feature_loss = 0.0
        if self.feature_distiller:
            # å‡è®¾åŒè§†è§’è’¸é¦æ—¶ï¼Œåªä½¿ç”¨ç¬¬ä¸€ä¸ªè§†è§’è¿›è¡Œç‰¹å¾åŒ¹é…
            feature_loss = self.feature_distiller(xa, xa)

        # æœ€ç»ˆæŸå¤±ç»„åˆ: Hard + Soft + Feature
        # alpha æ§åˆ¶ç¡¬æŸå¤±æƒé‡ï¼Œbeta æ§åˆ¶ç‰¹å¾æŸå¤±æƒé‡ï¼Œè½¯æŸå¤±æƒé‡ç”± 1-alpha-beta åŠ¨æ€å†³å®š
        soft_weight = max(1.0 - self.alpha - self.beta, 0.0)
        
        total_loss = self.alpha * hard_loss + soft_weight * soft_loss + self.beta * feature_loss
        return total_loss